{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivos disponibles:\n",
      "PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')\n",
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "WARNING:tensorflow:From C:\\Users\\migue\\AppData\\Local\\Temp\\ipykernel_18800\\328158102.py:9: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "\n",
      "‚úÖ TensorFlow est√° usando la GPU.\n",
      "GPU en uso: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Ver dispositivos disponibles\n",
    "print(\"Dispositivos disponibles:\")\n",
    "for device in tf.config.list_physical_devices():\n",
    "    print(device)\n",
    "\n",
    "# Ver si TensorFlow est√° usando GPU\n",
    "if tf.test.is_gpu_available():\n",
    "    print(\"\\n‚úÖ TensorFlow est√° usando la GPU.\")\n",
    "    print(\"GPU en uso:\", tf.config.experimental.list_physical_devices('GPU'))\n",
    "else:\n",
    "    print(\"\\n‚ùå TensorFlow NO est√° usando la GPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, False)  # Usar toda la memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, applications\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import GlobalAveragePooling2D, Activation, Dense\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import functools\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREACION DE DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              image_path      label\n",
      "86995  ../data/morton14k\\f_youtubeHTML5_1_9880.bin_mo...  streaming\n",
      "86996  ../data/morton14k\\f_youtubeHTML5_1_9888.bin_mo...  streaming\n",
      "86997  ../data/morton14k\\f_youtubeHTML5_1_990.bin_mor...  streaming\n",
      "86998  ../data/morton14k\\f_youtubeHTML5_1_9959.bin_mo...  streaming\n",
      "86999  ../data/morton14k\\f_youtubeHTML5_1_9987.bin_mo...  streaming\n",
      "       Cantidad  count\n",
      "0          chat  14500\n",
      "1         email  14500\n",
      "2         voice  14500\n",
      "3     streaming  14500\n",
      "4  filetransfer  14500\n",
      "5           p2p  14500\n"
     ]
    }
   ],
   "source": [
    "# Ruta al directorio donde est√°n las im√°genes\n",
    "data_dir = \"../data/morton14k\"\n",
    "\n",
    "# Definir reglas de clasificaci√≥n\n",
    "CLASS_RULES = {\n",
    "    \"chat\": [\"chat\"],\n",
    "    \"email\": [\"email\"],\n",
    "    \"voice\": [\"audio\", \"voipbuster\"],\n",
    "    \"streaming\": [\"vimeo\", \"netflix\", \"spotify\", \"youtube\", \"video\"],\n",
    "    \"filetransfer\": [\"sftp\", \"ftps\", \"files\", \"file\"],\n",
    "    \"p2p\": [\"bittorrent\"]\n",
    "}\n",
    "\n",
    "# Funci√≥n para asignar una etiqueta seg√∫n el nombre del archivo\n",
    "def get_label(filename):\n",
    "    for category, keywords in CLASS_RULES.items():\n",
    "        if any(keyword in filename.lower() for keyword in keywords):\n",
    "            return category\n",
    "    return None  # Si no coincide con ninguna categor√≠a\n",
    "\n",
    "# Recorrer la carpeta y subcarpetas para extraer informaci√≥n\n",
    "image_paths = []\n",
    "labels = []\n",
    "\n",
    "for root, _, files in os.walk(data_dir):  # Recorre todas las carpetas y archivos\n",
    "    for file in files:\n",
    "        if file.endswith(\".png\"):  # Solo archivos PNG\n",
    "            label = get_label(file)\n",
    "            if label:  # Solo a√±adimos im√°genes con una categor√≠a v√°lida\n",
    "                image_paths.append(os.path.join(root, file))  # Ruta completa\n",
    "                labels.append(label)\n",
    "\n",
    "# Crear DataFrame\n",
    "df = pd.DataFrame({\"image_path\": image_paths, \"label\": labels})\n",
    "\n",
    "# Mostrar las primeras filas para verificar\n",
    "print(df.tail())\n",
    "print(df[\"label\"].value_counts().to_frame().reset_index().rename(columns={\"index\": \"Etiqueta\", \"label\": \"Cantidad\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 3) [1. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Crear un diccionario para convertir etiquetas en n√∫meros\n",
    "label_map = {label: idx for idx, label in enumerate(df[\"label\"].unique())}\n",
    "df[\"label\"] = df[\"label\"].map(label_map)  # Convertimos a n√∫meros\n",
    "\n",
    "# Funci√≥n para cargar im√°genes y etiquetas\n",
    "# Funci√≥n para cargar im√°genes y convertir etiquetas a one-hot encoding\n",
    "def load_image_label(path, label):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_png(img, channels=3)  # Convertir PNG a tensor\n",
    "    img = tf.image.resize(img, (32, 32))  # Asegurar tama√±o correcto\n",
    "    img = img / 255.0  # Normalizaci√≥n [0,1]\n",
    "\n",
    "    label = tf.one_hot(label, depth=6)  # Convertir etiqueta a one-hot\n",
    "    return img, label\n",
    "\n",
    "\n",
    "# Convertir el DataFrame en un Dataset de TensorFlow\n",
    "dataset = tf.data.Dataset.from_tensor_slices((df[\"image_path\"].values, df[\"label\"].values))\n",
    "dataset = dataset.map(load_image_label)\n",
    "\n",
    "# Mostrar un ejemplo\n",
    "for img, label in dataset.take(1):  \n",
    "    print(img.shape, label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribuci√≥n de clases en el dataset original:\n",
      "\n",
      "label\n",
      "0    14500\n",
      "1    14500\n",
      "2    14500\n",
      "3    14500\n",
      "4    14500\n",
      "5    14500\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribuci√≥n de clases en el dataset de entrenamiento:\n",
      "label\n",
      "1    11600\n",
      "2    11600\n",
      "5    11600\n",
      "0    11600\n",
      "4    11600\n",
      "3    11600\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribuci√≥n de clases en el dataset de validaci√≥n:\n",
      "label\n",
      "1    2900\n",
      "2    2900\n",
      "5    2900\n",
      "4    2900\n",
      "3    2900\n",
      "0    2900\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total im√°genes en entrenamiento: 69600\n",
      "Total im√°genes en validaci√≥n: 17400\n"
     ]
    }
   ],
   "source": [
    "## SEPARACION DE TRAINING Y VALIDATION DATA\n",
    "\n",
    "# Ver distribuci√≥n de clases antes de dividir\n",
    "print(\"Distribuci√≥n de clases en el dataset original:\\n\")\n",
    "print(df[\"label\"].value_counts())\n",
    "\n",
    "# Dividir en entrenamiento y validaci√≥n asegurando la estratificaci√≥n\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, stratify=df[\"label\"], random_state=42)\n",
    "\n",
    "# Verificar distribuci√≥n despu√©s de dividir\n",
    "print(\"\\nDistribuci√≥n de clases en el dataset de entrenamiento:\")\n",
    "print(train_df[\"label\"].value_counts())\n",
    "\n",
    "print(\"\\nDistribuci√≥n de clases en el dataset de validaci√≥n:\")\n",
    "print(val_df[\"label\"].value_counts())\n",
    "\n",
    "# Crear datasets de TensorFlow\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_df[\"image_path\"].values, train_df[\"label\"].values))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_df[\"image_path\"].values, val_df[\"label\"].values))\n",
    "\n",
    "# Aplicar la funci√≥n de carga de im√°genes\n",
    "train_dataset = train_dataset.map(load_image_label)\n",
    "val_dataset = val_dataset.map(load_image_label)\n",
    "\n",
    "# Contar elementos en los datasets de TensorFlow\n",
    "train_count = sum(1 for _ in train_dataset)\n",
    "val_count = sum(1 for _ in val_dataset)\n",
    "\n",
    "print(f\"\\nTotal im√°genes en entrenamiento: {train_count}\")\n",
    "print(f\"Total im√°genes en validaci√≥n: {val_count}\")\n",
    "\n",
    "# Mezclar, agrupar en batches y optimizar\n",
    "batch_size = 64\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_dataset = train_dataset.shuffle(1000).batch(batch_size).prefetch(AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(batch_size).prefetch(AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACTUAL MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 15\u001b[0m\n\u001b[0;32m      4\u001b[0m EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m      6\u001b[0m label_map \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;241m0\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchat\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;241m1\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124memail\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;241m5\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp2p\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     13\u001b[0m }\n\u001b[1;32m---> 15\u001b[0m data_augmentation \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[0;32m     16\u001b[0m     layers\u001b[38;5;241m.\u001b[39mRandomFlip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhorizontal_and_vertical\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     17\u001b[0m     layers\u001b[38;5;241m.\u001b[39mRandomRotation(\u001b[38;5;241m0.2\u001b[39m),\n\u001b[0;32m     18\u001b[0m     layers\u001b[38;5;241m.\u001b[39mRandomZoom(\u001b[38;5;241m0.2\u001b[39m),\n\u001b[0;32m     19\u001b[0m ])\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# MODELO\u001b[39;00m\n\u001b[0;32m     23\u001b[0m model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[0;32m     24\u001b[0m     layers\u001b[38;5;241m.\u001b[39mRescaling(\u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m),\n\u001b[0;32m     25\u001b[0m     \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     45\u001b[0m     layers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;28mlen\u001b[39m(label_map), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Usar len(label_map) para las 6 clases\u001b[39;00m\n\u001b[0;32m     46\u001b[0m ])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "# Par√°metros\n",
    "IMG_SIZE = (32, 32)\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 3\n",
    "\n",
    "label_map = {\n",
    "    0: 'chat',\n",
    "    1: 'email',\n",
    "    2: 'voice',\n",
    "    3: 'streaming',\n",
    "    4: 'filetransfer',\n",
    "    5: 'p2p'\n",
    "}\n",
    "\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    layers.RandomRotation(0.2),\n",
    "    layers.RandomZoom(0.2),\n",
    "])\n",
    "\n",
    "# MODELO\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    # No usamos data aug porque no funciona bien\n",
    "    layers.Rescaling(1./255),\n",
    "    \n",
    "    layers.Conv2D(64, (3,3), activation='relu', padding='same', input_shape=(32, 32, 1)),  # Cambiar 1 a 3 para 3 canales\n",
    "    layers.Conv2D(64, (3,3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    layers.Conv2D(128, (3,3), activation='relu', padding='same'),\n",
    "    layers.Conv2D(128, (3,3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    layers.Conv2D(256, (3,3), activation='relu', padding='same'),\n",
    "    layers.Conv2D(256, (3,3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    layers.Conv2D(512, (3,3), activation='relu', padding='same'),\n",
    "    layers.Conv2D(512, (3,3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    layers.Flatten(),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(len(label_map), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entrenamiento\n",
    "model.fit(train_dataset, validation_data=val_dataset, epochs=EPOCHS)\n",
    "\n",
    "# Evaluaci√≥n del modelo en el conjunto de validaci√≥n\n",
    "val_labels = np.array(val_df[\"label\"])  # Etiquetas reales\n",
    "predictions = model.predict(val_dataset)\n",
    "pred_labels = np.argmax(predictions, axis=1)  # Etiquetas predichas\n",
    "\n",
    "# Matriz de Confusi√≥n\n",
    "conf_matrix = confusion_matrix(val_labels, pred_labels)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap=\"Blues\", xticklabels=label_map.keys(), yticklabels=label_map.keys())\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Reporte de clasificaci√≥n\n",
    "print(\"\\nüìä Classification Report:\\n\")\n",
    "print(classification_report(val_labels, pred_labels, target_names=label_map.keys()))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pito",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
